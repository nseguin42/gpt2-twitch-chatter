# gpt2-twitch-chatter

A GPT2 model fine tuned on Twitch chat logs, plus a pipeline for generating chat messages. Based on Huggingface transformers.

## Files

### `notebook.ipynb`

This is a Jupyter notebook that I used to develop this project. It contains the code for each step as well some demos and discussion. The Python files contain roughly the same code in a more streamlined one-shot form suitable for running from the command line.

### `parse_logs.py`

This script parses the raw Twitch chat logs which are stored in `data/logs`, and saves them as a .csv in `data/raw`. The log files should be formatted in the way used by [logs.ivr.fi](https://logs.ivr.fi/). For example, messages have the form

    [YYYY-MM-D HH:MM:SS] #channel user: message

Note the day is not zero-padded. Bans and timeouts look like e.g.

    [YYYY-MM-D HH:MM:SS] #channel user has been timed out for n seconds
    [YYYY-MM-D HH:MM:SS] #channel user has been banned

The generated .csv files have the format

    date,time,channel,username,message

### `tokenize.py`

This script tokenizes the parsed logs generated by `parse_logs.py`. It truncates lines longer than 512 tokens, groups together smaller lines into batches of up to 512 tokens, and pads the batches to 512 tokens with a new special token.

The resulting blocks are saved as a Dataset (using the Huggingface `datasets` module) and shuffled. Then 10% of the blocks are split off into a test Dataset. Both datasets are added to a DatasetDict which is saved to `data/tokenized_datasets`.

### `train.py`

This is the actual training script. It loads the pretrained `gpt2-medium` model from Huggingface `transformers`, and fine tunes it on the tokenized chat logs. The model is saved to `models/gpt2-twitch-chatter`.

The pretrained model has its input embeddings resized to the new `vocab_size` of 50257, since we added a padding token. The maximum sequence length is also decreased to 512.

The trainer uses DeepSpeed / Zero3 to offload some data into CPU memory and perform other optimizations. This dramatically increases the maximum batch size we can use. We also use fp16 to further increase the batch size. A batch size of 24 is used by default, which should be suitable for a single v100 GPU and 32GB of RAM.

The model is trained with the `torch_adamw` optimizer with a `learning_rate` of 1e-3 and a linear warmup period of 0.1 epochs. The training is done for 3 epochs, with a batch size of 8. A checkpoint is saved after each epoch.

### `generate.py`

This script is the basis of a generation pipeline. It loads the fine tuned model from `model`, and uses it to generate a single chat message.

The script optionally takes a prompt as input. If provided, the prompt is tokenized and passed to the model. The model will then produce what it thinks the next line of chat should be.
